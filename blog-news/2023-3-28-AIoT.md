# Li Xiang, CTO of Edgenesis, talked about the integration of cloud native, AI and Internet of Things



## Cloud Native Enters Edge Scenarios
The term "cloud native" is probably familiar to most people.  Now it is becoming very popular. As cloud service providers have gradually taken over various fields, the concept of cloud native has also been introduced into other areas. In simple terms, it brings us four main characteristics:
Firstly, high scalability. Cloud native can flexibly scale according to our needs, ensuring that resources are not wasted or lacking.
Secondly, high flexibility. It decouples the previously highly coupled systems to form small microservices one by one. In this way, the services are actually modularized. If there is a problem somewhere and what needs to be added, you can only develop this microservice without worrying about other services.
Thirdly, high adaptability. The cloud native development concept requires that our services can adapt to most software and hardware environments, which means that it is platform-agnostic and not affected by external conditions.
Finally, high agility. We need to quickly respond to any problems and needs, and continuously develop and deploy. Server and container orchestration technologies are actually rooted in the cloud native concept. For enterprises, the benefits it brings are better cost reduction and increased efficiency.
Cloud service providers provide us with basic hardware and technical software, so we don't need to spend time and effort thinking about how to buy and configure hardware, which greatly reduces our costs. Additionally, relying on unified development tools reduces the learning curve for software developers. Containerization increases efficiency, and most cloud native developers actually use agile development principles as they complement each other.
Every time you want to develop something, you only need to pay attention to this part of the microservice and change the code of this part, without thinking about whether it will cause a direct collapse of the entire software. Since it is difficult to happen in a highly decoupled system of microservices, there is no need to care about the impact of irrelevant services, and we only need to focus on the problem itself.
In this way, the cost of design and development will be reduced a lot. When we update software, the microservice architecture and cloud computing vendors provide a lot of redundancy. The software update will not cause the service to be offline for a long time, and the user experience becomes very high.
There are six core concepts of cloud native architecture.
1. Immutable infrastructure. Simply put, it is to ensure that the server is replaceable, or that the hardware is replaceable. When we design software, we don't need to consider the impact of software changes on hardware. We can treat it as if the hardware does not exist, or the hardware is always suitable for the software.
2. Microservices. Microservice is also a very popular term. Each microservice is independent of each other, which reduces a lot of burden during development.
3. Service mesh. The service mesh is actually an imperceptible communication layer. In other words, when we develop microservices, we only need to ensure that the microservices themselves can interact. As for how they interact, how microservices communicate with each other, and how microservices communicate with users, we donâ€™t need to worry about it. The service mesh automatically handles it for us.
4. container. Container is also a very popular word. It can bring us a very light environment, little waste of resources, and improved scalability. Simply put, it is the effect of cross-platform. We only need to package the service into a container image, so it doesn't matter where and how it runs.
5. Continuous Integration and Continuous Deployment, or CI/CD. It provides us with a complete set of development and deployment processes, we only need to upload the code. The rest, including compilation, testing, and deployment, are all done by CI/CD. We don't need to care at all.
6. API. API is the core for users to interact with software. It is the cornerstone of all communication. In fact, using software is essentially adjusting API.

These six concepts can be synthesized into a simple architecture. The user first calls the API, and then the API call information is transmitted to each microservice through the service grid, and then processed. If there is a new version to be updated, the new version will be deployed to each microservice via CI/CD to ensure the smooth update of the service. The underlying computing resource (immutable infrastructure) can be completely ignored. It is there and will always support us.

According to the data given by CNCF (cloud native computing foundation), it can be found that the number of contributors, members, end users, and projects is increasing year by year, and the growth rate is getting faster and faster. So we can say that the expansion of cloud native in various fields is unstoppable trend.

Many famous projects, such as kubernetes, etcd and helm. These familiar projects are actually hatched by cloud native, and they are indeed widely used in various fields and occupy a very important position.



## AIoT is gradually becoming popular in edge scenarios


With the continuous growth of edge computing power, we can put AI work on the edge. So AIoT is gradually becoming a very popular concept. Once edge computing is mentioned, it is clear that this is the world of IoT.

Now there is the word AIoT, or AIIoT, because AIoT is Artificial Intelligence of Things. What we are talking about now is the Internet of Everything between devices, so it is AI and IoT. The current trend is: first of all, the equipment itself is becoming more and more intelligent. AI computing is also gradually sinking to the edge. Many devices themselves have been open sourced, and their own APIs and even design drawings have also been open sourced. Therefore, we will encounter more and more devices that allow us to freely program and interact freely in the field of the Internet of Things.
We find that many developers in the offline community are even making this kind of artificial intelligence device by themselves. There are more and more intelligent hardware, which also brings us more AIoT requirements. Therefore, it is very suitable for AIoT to further develop vigorously.
The popularity of ChatGPT around the world has actually brought us a lot of inspiration: what are humans doing when interacting with devices? In fact, it is the communication between people and equipment. Pressing this button is telling the device: you should perform this action. Then the green light of the device feedback tells us that the operation is completed. So it's actually communicating.

Then if you use GPT3 or a similar LP model to empower the device, you will find that we even don't need to read the manual to learn what this button does. We just need to give commands to the device and say to it in natural language: "Please help me do this". The device will reply in natural language: "I'm done". This is very beneficial to the improvement of user experience and the reduction of learning costs.

Cloud Native actually provides AIoT with many very useful features. It can even be said that Cloud Native developed these features for AIoT. For example, the Kuberbetes-centric orchestration system provides a CPU management strategy that can directly isolate the CPU. This means that CPU resources are guaranteed to be efficiently utilized directly in CPU-heavy tasks. This ensures that the CPU does not interfere with AI. As we all know, GPU is used in AI. So the device plug-in provided by Kuberbetes allows the application to have the ability to use GPU. Topology management policies are used to ensure efficient allocation of resources to GPUs and CPUs. Pod and Operator are used to ensure mutual independence and automatic operation of microservices. These are a series of characteristics that are very suitable for AIoT to operate at the edge.

## Shifu empowers cloud-native AIoT

Therefore, Edgenesis has developed Shifu, a cloud-native AIoT development platform, with the purpose of combining the features of cloud-native and AIoT just mentioned to release greater energy.
Shifu is a development framework based on Kuberbetes. Its core technology or core feature is structural virtualization. Shifu virtualizes each device into each microservice. These microservices are an API for users, and users can directly call the device by calling the API. At the same time, as these APIs are unified, users can manipulate all devices without any learning costs at all, which is a great interactive capability.

The picture above is the technical architecture of Shifu. It can be seen that the underlying physical devices are real devices, which have been transformed from Shifu into abstract virtual devices. A bunch of physical device clusters can be abstracted into virtual device clusters, and the entire scene becomes a programmable scene. Thus there are things like virtual laboratories and virtual factories. There is such a security framework inside the thing itself, which can ensure the security of the system. The management and control framework can ensure the smooth operation of device connection and interaction, and the interconnection framework can ensure cooperation between devices. The platform framework can be operated with UI, and the automatic scheduling system performs equipment automation and resource scheduling.

Shifu provides a complete set of solutions from access equipment to whole system operation and maintenance. First of all, Shifu is compatible with most protocols and drivers on the market, and can be plug and play without ecological barriers. In the application development part, Shifu actually provides users with a series of APIs, which can be called by users themselves, ensuring that users can call any device very simply. The last link is system operation and maintenance, which is k8s native architecture. Kuberbetes itself is a very good operation and maintenance tool. In view of the application of this micro-service architecture and a unified API, it is also very simple in operation and maintenance, providing ultra-high system stability. This shows that Shifu is a complete solution from 0 to 100.

We define Shifu as a bridge connecting the cloud and devices. Among the open source projects, there are products such as K3s, Edge X, and EMQ, which also serve the Internet of Things. In fact, their features are not exactly the same as those of Shifu. Shifu uses cloud-native integrated architecture to directly manage devices.

So, that's a very good competitive advantage for Shifu. The main use of Shifu is actually in the field of industrial intelligent manufacturing. Shifu can first ensure that all devices are connected, and then realize the intelligence of the entire scene, and make development faster. Shifu provides manufacturers or customers with this complete set of solutions, which will solve a series of pain points for them in the cloud, on the edge side, and in its equipment or industrial site access, equipment management, and operation and maintenance.

Specifically, we first realized very good cloud-side collaboration. Shifu can do some complex operations on the cloud. At the edge, Shifu can do things such as the deployment of edge AI computing power and the deployment of edge AI applications to ensure that the communication between the cloud and others is smooth. Secondly, Shifu ensures 100% compatibility with the equipment, and the three parts of the cloud, edge, and terminal can communicate with each other. If there is no cloud or if you don't want to connect to the Internet, you can also ensure that it is only deployed on the edge side and the device side, which is also a very good and complete privatization solution.

The effect achieved by Shifu is to reduce the complexity of connecting this traditional control architecture (such as the industrial software ANDON, EMS, and SCADA in the picture) with the underlying controller equipment. On a real industrial site, the contact methods of heterogeneous equipment PLC and HMI are different, so we need to consider how to connect to the MES system. These require a high degree of customization, and it affects the whole body, making it very difficult to reuse.
Shifu is used to improve this problem. Shifu can be compatible with all these services in the lower part. What is open to the north is a complete and unified API, which does not require any learning cost, but only needs to be called. In fact, we are open to HTTP. For example, we can directly call the HTTP API when calling, which is a very good unified solution provided by Shifu.


## Application cases of Shifu



The Shifu framework has been implemented correctly and widely by many customers.

The first example is the Industry 4.0 liquid laboratory. The customer has a series of laboratories with a bunch of liquid dispensers, MicroplateReader, liquid quantifiers, automatic guided vehicles, robotic arms and other things. Do we need to know what these things are for? In fact, we don't need to know what they are, we just need to know what capabilities they have. For example, if an instrument provides the ability to shake test tubes, Shifu can abstract it into an API. The action of shaking is an API that users can call.
What Shifu does is turn the device itself into a series of APIs. One interesting thing is that the robotic arm and automatic guided vehicle can actually be assembled together. And we actually put the two together in Shifu. These two are a digital completion or a microservice, which is a relatively advanced capability that Shifu can provide - the aggregation of devices.
The whole process is to install the robotic arm on the guide car, and then send these test tubes to the liquid dispenser, then to the microplatereader and the oscillator for processing. Finally, take it down and send it to another machine. Shifu controls this set of processes. It manages and arranges all equipment, and then runs the user's automated production logic.

Shifu has Shifu Controller, which is equivalent to a center. These node users can define themselves. The processing unit contains things that process liquids. MovingArm is an aggregation that manages the movement of automatic guided vehicles and robotic arms together. In fact, this is very simple architecture. Shifu can support edge platforms such as Prometheus, SQL, and MQ to apply to underlying data.

Here's how users can use Shifu. The user first provides the driver file (provided by the device manufacturer). No matter what format the file is, whether it is an executable file, a Python Library, or a dynamic link library, it doesn't matter. The user directly submits it to the Shifu platform, and Shifu can package it, turn it into something that can be used.
The next step is to configure the file. The information that the user needs to fill in includes: the hardware device sku, the name of the hardware device, the name of the driver image, how the device is connected, the name of its connection method, and which APIs it needs to use. As far as the detection method is concerned, the automatic setting of the finite state machine is optional, and what to fill in is determined according to the needs of the user.
To put it simply, the finite state machine is a relatively high-level feature. It is used to realize simple automation, which determines in which state the device receives another command and which new state it will transfer to. In fact, it is a feature of automatic response based on the current environment and current commands.

The second example is the first round of cooperation between Shifu and Chinese ships. Shifu's biodegradation system was deployed on China's first autonomous large tanker. The bio-line system of the "first ship" is installed on a part of each ship, and then there will be many bio-chain systems on board to deal with food waste. We deployed the Shifu Framework on the ship to ensure that it can be upgraded or changed quickly and quickly, as well as the biological professional system.

Here is a solution for delivering an independent cluster on the cloud side. First, we use k8s on the cloud to collect all the data on the ship for analysis, and then deploy k3s on the edge (that is, on the ship), which can produce some simple applications on the edge, including some small AI processing like real-time alarms, electromechanical control.

The third example is the second cooperation between Edgenesis and Chinese ships to realize cloud edge-to-end collaboration. We used Ali's OpenYurt to get through the cloud edge. In this way, we can ensure that this system can not only serve every ship, but also serve the whole fleet of ships, or all ships in the world.

The last example is the AIoT remote operation and maintenance scenario based on AR equipment provided by Edgenesis to a Chinese central State-owned enterprises. The above picture is a scene of AR inspection. Our goal is to use AI to guide and detect workers' operations. To put it simply, by wearing MR helmets, workers can operate equipment to learn in real time according to the prompts given to them by MR helmets. The MR helmet will also detect whether the operation is wrong. Besides, it will look at the surrounding environment to tell the worker what to do next.
Shifu virtualizes MR helmets and machines (including sensors) in the scene, and then ensures that AI uses enough resources to process these things without any waste.

The software architecture is very similar to the liquid laboratory mentioned earlier. The only difference is that it has an extra AI core. There is an AI core at the edge that can process the information sent to him by all MR helmets in the scene. There is also an AI on Device (airborne AI/device-side AI) on the MR helmet, which can assist the device in handling the current operating conditions of workers. The equipment connected to Shifu is the digital twin of the MR helmet, the sensors on the machine, the environmental sensors and the camera.

The whole workflow is as follows: First, the workers put on the MR helmet in this scene. The helmet will show the operation process in front of the worker's eyes, and the worker will follow the operation process to process. Then AI on Device will judge whether the worker's operation is correct. If it is correct, proceed to the next step. Otherwise, it will be stopped immediately, and then other processing will be performed.
Shifu will record all detected operations. What the camera captures in the form of video streams or photos will also be recorded. If necessary, those information will be uploaded to the cloud or uploaded to the edge. Engineers can also remotely connect to the deviceShifu of the MR helmet and start a video call. Video calls are two-way calls. The engineer can see the worker's POV, and the worker can also see the engineer's video call interface, which is also obtained through video streaming through deviceShifu.
Shifu itself will also collect all data from helmets, devices, sensors, cameras and other devices, and transmit them to the edge side and the cloud for analysis. This data includes everything, like readings, text patterns, audio and video, everything. Finally, the cloud or edge AI will upload the results of its own analysis to the Shifu platform database. When necessary, manufacturers can use deviceShifu to tell workers the next step of operation and other higher-level prompts, or stop related equipment to ensure that the operation will not affect other equipment.
The above three cases mainly describe what kind of cloud native and AIoT help the Shifu platform provides for liquid laboratories, ships and MR workshops. As you can see, customers benefit a lot from this. We also hope that this ability of Shifu can be applied to more AIoT scenarios to help everyone reduce costs and increase efficiency.